{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba83fd76-ad52-4951-8da6-b1f772fc2a29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c9b0d97-e9b2-4090-b6e5-7957d473b8f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 256, 3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "Image = torch.randn((256,256,3))\n",
    "Scale = torch.tensor([0.5, 1.5, 1])\n",
    "\n",
    "print(Image.shape)\n",
    "print(Scale.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3c82fa6-bd83-47ac-8cd2-119e128925f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3074,  0.8897, -1.0785],\n",
       "         [ 0.1653,  0.0955,  0.4745],\n",
       "         [-0.8885, -0.5066,  0.4441],\n",
       "         ...,\n",
       "         [-0.6530,  0.0617, -1.4570],\n",
       "         [ 0.7606,  1.0855, -1.0595],\n",
       "         [-0.4463, -0.5083, -1.1160]],\n",
       "\n",
       "        [[-1.6276, -0.8496, -1.8173],\n",
       "         [-0.4902, -0.0084,  1.6516],\n",
       "         [-1.0016,  0.4214, -1.9938],\n",
       "         ...,\n",
       "         [ 0.1142, -0.4086, -0.0222],\n",
       "         [ 0.9769,  0.2022,  2.1875],\n",
       "         [ 0.6793, -0.4075, -0.5223]],\n",
       "\n",
       "        [[-1.1438,  1.0402, -1.0392],\n",
       "         [ 1.0573, -1.8691,  1.0115],\n",
       "         [-0.2694,  0.3307,  1.2699],\n",
       "         ...,\n",
       "         [ 1.5941,  1.4739,  1.2558],\n",
       "         [-0.0762, -0.4922, -0.3753],\n",
       "         [ 0.6467, -1.3935, -1.2177]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.5051, -1.1134,  0.6332],\n",
       "         [-0.6970,  0.8001,  0.2443],\n",
       "         [-1.5832, -0.9591,  0.2247],\n",
       "         ...,\n",
       "         [ 0.7673, -0.4148,  0.1455],\n",
       "         [ 1.7792,  0.9201,  0.2438],\n",
       "         [ 0.5394, -0.2879, -0.4208]],\n",
       "\n",
       "        [[ 0.2616, -1.4586, -0.3296],\n",
       "         [-0.9583,  0.1158,  1.2332],\n",
       "         [ 0.2835,  1.3421,  0.1408],\n",
       "         ...,\n",
       "         [-1.2000,  1.7803,  0.2809],\n",
       "         [-1.5886,  0.6544, -0.9693],\n",
       "         [-0.1537,  0.1938,  1.5082]],\n",
       "\n",
       "        [[-1.2005, -0.9272, -0.7631],\n",
       "         [-0.4119,  1.0350,  1.6126],\n",
       "         [-0.4823,  0.0595, -0.2264],\n",
       "         ...,\n",
       "         [-1.3055,  0.7882, -1.1754],\n",
       "         [-0.4663,  0.2802, -1.1742],\n",
       "         [ 0.2379, -0.8139, -0.1418]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2c8e2bc-76ef-4fff-b1e9-dd5a82ffbea9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1537,  1.3346, -1.0785],\n",
       "         [ 0.0826,  0.1433,  0.4745],\n",
       "         [-0.4442, -0.7599,  0.4441],\n",
       "         ...,\n",
       "         [-0.3265,  0.0925, -1.4570],\n",
       "         [ 0.3803,  1.6282, -1.0595],\n",
       "         [-0.2231, -0.7624, -1.1160]],\n",
       "\n",
       "        [[-0.8138, -1.2744, -1.8173],\n",
       "         [-0.2451, -0.0125,  1.6516],\n",
       "         [-0.5008,  0.6321, -1.9938],\n",
       "         ...,\n",
       "         [ 0.0571, -0.6129, -0.0222],\n",
       "         [ 0.4885,  0.3033,  2.1875],\n",
       "         [ 0.3396, -0.6113, -0.5223]],\n",
       "\n",
       "        [[-0.5719,  1.5603, -1.0392],\n",
       "         [ 0.5286, -2.8036,  1.0115],\n",
       "         [-0.1347,  0.4961,  1.2699],\n",
       "         ...,\n",
       "         [ 0.7971,  2.2109,  1.2558],\n",
       "         [-0.0381, -0.7383, -0.3753],\n",
       "         [ 0.3234, -2.0902, -1.2177]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.7525, -1.6701,  0.6332],\n",
       "         [-0.3485,  1.2001,  0.2443],\n",
       "         [-0.7916, -1.4386,  0.2247],\n",
       "         ...,\n",
       "         [ 0.3837, -0.6222,  0.1455],\n",
       "         [ 0.8896,  1.3802,  0.2438],\n",
       "         [ 0.2697, -0.4319, -0.4208]],\n",
       "\n",
       "        [[ 0.1308, -2.1879, -0.3296],\n",
       "         [-0.4791,  0.1737,  1.2332],\n",
       "         [ 0.1417,  2.0132,  0.1408],\n",
       "         ...,\n",
       "         [-0.6000,  2.6705,  0.2809],\n",
       "         [-0.7943,  0.9815, -0.9693],\n",
       "         [-0.0769,  0.2907,  1.5082]],\n",
       "\n",
       "        [[-0.6002, -1.3908, -0.7631],\n",
       "         [-0.2060,  1.5525,  1.6126],\n",
       "         [-0.2412,  0.0893, -0.2264],\n",
       "         ...,\n",
       "         [-0.6528,  1.1823, -1.1754],\n",
       "         [-0.2332,  0.4202, -1.1742],\n",
       "         [ 0.1190, -1.2208, -0.1418]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image*Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b55bb6ef-3d74-4806-877c-206471c4b72b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = torch.arange(20, dtype = float).reshape(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0e0ed99-21bf-4bc8-ae30-2abded066b40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e962360-d391-47f5-9fab-50d0dc9ad521",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]], dtype=torch.float64),\n",
       " tensor([ 8.,  9., 10., 11.], dtype=torch.float64))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t, torch.mean(t, axis = 0) # Mean across the rows, that are the first dimension!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc0f03ae-82fc-46bd-b7e7-a18e4606a637",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5000,  5.5000,  9.5000, 13.5000, 17.5000], dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t, axis = 1) # Mean across the columns, that are the SECOND dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f59a7537-a85a-4eea-bd2b-e2e2ee0406f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = torch.randn(5, 256, 256, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ff1a7a-870d-47dc-9dec-3b2d732f38e8",
   "metadata": {},
   "source": [
    "Take the mean across the batch (size 5), different images!\n",
    "\n",
    "It will take the mean of the batchs. For example, it will look each individual channel (red, green, blue) and compute the mean of these channels using the different images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "789f8c43-91ea-4bd5-a502-a7155c35dca2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 256, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t, axis = 0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a83477-69a5-4212-bd9f-6fc33a977081",
   "metadata": {},
   "source": [
    "Take the mean across the color channels. It could be used to get something like the brightness of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1148594-2ea8-47d7-a47c-db886ba1a985",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 256, 256])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t, axis = -1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7e6576-6f4c-44ea-91d5-628e8d35ae6c",
   "metadata": {},
   "source": [
    "# Where Pytorch Differs from Numpy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ca33f6a-81b0-40ea-a73f-c3d162402a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([[5.,8.],[4.,6.]], requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2fce1bd-707c-4c40-99f5-e5608c5acf3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 8.],\n",
       "        [4., 6.]], requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b675550-95ee-4eff-a758-99706811b83e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = x.pow(3).sum() + x.pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab2c2c32-bcd0-43c6-8be5-5b7854e6687b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 85., 208.],\n",
       "        [ 56., 120.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*x.pow(2) + 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad563f98-f664-46b1-b07e-5022eac75531",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1058., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "884a771b-f8cd-4571-b97c-7527bb24ace1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 85., 208.],\n",
       "        [ 56., 120.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628317c5-21b2-4be2-a9c9-ad9863031ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
